{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Homework_DL_assignment.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.3"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ELehmann91/FS1/blob/master/Homework_DL_assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9vUiQI5yaxvZ",
        "colab_type": "text"
      },
      "source": [
        "# Todos\n",
        "\n",
        "### # (1) You predict one week into the future now, instead of 1 day. Predicting on a more aggregate level removes noise and will thus increase prediction accuracy.\n",
        "\n",
        "# Predicting Blabla\n",
        "\n",
        "## Dataset\n",
        "\n",
        "[Beijing PM2.5 Data Data Set](https://archive.ics.uci.edu/ml/datasets/Beijing+PM2.5+Data)\n",
        "\n",
        "### Columns of the dataset:\n",
        "\n",
        "# Description of the data files\n",
        "\n",
        "*All data has been anonymized. You must nevertheless not share this data-set with any outsiders.*\n",
        "\n",
        "* The information is contained in two datafiles:\n",
        "    * [*solditems_encoded_stage2.csv* contains sales information](#information-on-product-sales)\n",
        "    * [*content_encoded_stage2.csv* contains information on product features](#information-on-the-properties-of-the-products)\n",
        "* Note that not all variables were originally specified for each column, but there are no NA (missing) values in the datafiles:\n",
        "    * For categorical variables, missing values include a 0 in all one-hot encoded columns of the variable.\n",
        "    * For numerical variables, missing values were simply filled with the column mean (the most simple imputation).\n",
        "\n",
        "## Information on product sales\n",
        "\n",
        "1) Datafile: **solditems_encoded_stage2.csv**\n",
        "2) Each row corresponds to the sale of one specific product token.\n",
        "3) Apart from the first two columns,\n",
        "    * each *numerical variable* has been normalized to the [0,1] interval;\n",
        "    * each *categorical variable* (including *binary variables*) has been one-hot encoded. \n",
        "\n",
        "Description of the columns:\n",
        "\n",
        "* `product_sid`: The unique identifier of a product model. You can think of this as the type, and the rows of the sales data correspond to tokens of these types.\n",
        "    * This identifier corresponds to the `ProductId` in the datafile describing properties of product models.\n",
        "* `created_date_id`: This is the date in 'YYYY-MM-DD' format when the specific product token was sold.\n",
        "* `sales_item_price_created`: The actual price at which the product token was sold.\n",
        "* `sales_item_price`:  The price of the product token.\n",
        "* `sales_voucher_created`: The actual price of a voucher for the product token.\n",
        "* `sales_voucher`: The price of a voucher for the product token.\n",
        "* `sales_value_created`: The actual price paid by the buyer.\n",
        "* `sales_value`: The price to be payed by the buyer.\n",
        "* `created_year`: The year of the sale.\n",
        "* `created_month`: The month of the sale.\n",
        "* `created_weekday`: The weekday of the sale *originally* encoded from 0 (Monday) to 6 (Sunday).\n",
        "* `days_since_first_sold`: The number of days since the frst sale happened\n",
        "* `days_since_release`: The number of days since the product was released.\n",
        "* `Ratio_SalesItemPrice_per_created`: Ratio of the *sales_item_price / sales_item_price_created* variables.\n",
        "* `Ratio_SalesValue_per_created`:  Ratio of the *sales_value / sales_value_created* variables.\n",
        "* `Ratio_SalesItemPrice_per_SalesValue`: Ratio of the *sales_item_price / sales_value* variables.\n",
        "* `Ratio_SalesItemPriceCreated_per_SalesValueCreated`: Ratio of the *sales_item_price_created / sales_value_created* variables.\n",
        "* `channel_sid_N`: One-hot encoded column of the *channel_sid* variable which specifies the channel (e.g., web browser) on which the sale happened.\n",
        "* `returned_date_id_N`:  One-hot encoded column of the *returned_date* variable which specifies if a product token was returned (N=1) or not (N=0).\n",
        "\n",
        "Note that the ratio values were included so that the original relation between the variables in question is not lost after scaling the columns.\n",
        "\n",
        "Note also that some of the columns have been pre-computed for your convenience only; if you like, you can compute them or other derived variables yourself.\n",
        "\n",
        "## Information on the properties of the products\n",
        "\n",
        "1) Datafile: **content_encoded_stage2.csv**\n",
        "2) Each row corresponds to a product model (that is, a type whose tokens are rows of the sales datafile).\n",
        "3) Apart from the first two columns,\n",
        "    * each *numerical variable* has been normalized to the [0,1] interval;\n",
        "    * each *categorical variable* (including *binary variables*) has been one-hot encoded. \n",
        "\n",
        "Description of the columns:\n",
        "\n",
        "* `ProductId`: The unique identifier of a product model.\n",
        "    * This identifier corresponds to the `product_sid` in the datafile describing product sales.\n",
        "* `ProductNameCleaned`: the product name without the color specification (encoded here as an integer). \n",
        "This means that several product models (with different product ID's) may be assigned to the same ProductNameCleaned.  \n",
        "* `ReleaseYear`, `ReleaseMonth`, `ReleaseDayOfWeek` encode the year, month and weekday (originally from 0=Monday to 6=Sunday) of the date when the product was released.\n",
        "* `DisplaySize_in`, `Storage_GB`, `CPUSpeed_GHz`, `DisplayHeight_px`, `DisplayResolution_MP`, `DisplayWidth_px`, `FrontCamResolution_MP`, \n",
        "`MainDescription_NumOfWords`, `Network_NumOfFreqs`, `RAM_GB` , `RearCamResolution_MP` , \n",
        "`Size_Depth` , `Size_Height` , `Size_Width` , `Weight` , `Ratio_HeightPerWidth` , `Ratio_DepthPerWidth` \n",
        "are numerical variables describing different properties of a product. Most of these are self-explanatory except maybe for the following:\n",
        "    * `MainDescription_NumOfWords` encoded the number of words in the textual description of the product.\n",
        "    * `Network_NumOfFreqs` encoded the number of frequencies the product is capable of handling.\n",
        "* The rest of the columns are one-hot-encoded columns of categorical variables capturing different properties of the products. Again, the names of the variables should be self-explanatory.\n",
        "    d direction -> Todo: Not that good\n",
        "\n",
        "**Iws:** Cumulated wind speed -> Todo: Not that good\n",
        "\n",
        "**Is:** Cumulated hours of snow\n",
        "\n",
        "**Ir:** Cumulated hours of rain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QK79-BbqP_nd",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9gQTXPmHVxNw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Code to read csv file into Colaboratory:\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "# Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-02T15:24:58.995243Z",
          "start_time": "2019-05-02T15:24:58.033983Z"
        },
        "id": "_HHPnxBYaxva",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!wget https://raw.githubusercontent.com/jbrownlee/Datasets/master/pollution.csv"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-02T15:25:00.287408Z",
          "start_time": "2019-05-02T15:24:59.003580Z"
        },
        "id": "gcgiWHw0axvh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!pip install seglearn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-02T15:25:00.659548Z",
          "start_time": "2019-05-02T15:25:00.294723Z"
        },
        "id": "hGlqTvV6axvl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd \n",
        "import csv\n",
        "import numpy as np \n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#import warnings\n",
        "#warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-02T15:25:00.665793Z",
          "start_time": "2019-05-02T15:25:00.661169Z"
        },
        "id": "raf0czhUaxvo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sniff_format(location):\n",
        "    with open(location, newline='') as csvfile:\n",
        "        sniffer = csv.Sniffer()\n",
        "        sample = csvfile.read(1024)\n",
        "        dialect = sniffer.sniff(sample)\n",
        "        header = sniffer.has_header(sample)\n",
        "        if header:\n",
        "            header=0\n",
        "        else:\n",
        "            header=None\n",
        "\n",
        "    return {\"dialect\":dialect, \"header\":header}\n",
        "\n",
        "def describe_full(df):\n",
        "    #pd.options.display.float_format = '{:.2f}'.format\n",
        "    dtypes_description=pd.DataFrame(dict(df.dtypes),[\"dtypes\"])\n",
        "    na_description = pd.DataFrame(dict(df.isna().sum()),[\"NA-s\"])\n",
        "    na_percent = ((pd.DataFrame(dict(df.isna().sum()),[\"NA%\"])/len(df))*100).round(decimals=2)\n",
        "    description = df.describe(include='all')\n",
        "    full_description = dtypes_description.append(na_description).append(na_percent).append(description).replace(np.nan, '', regex=True)\n",
        "\n",
        "    mask = full_description.loc[\"freq\",:]==1\n",
        "    full_description.at[[\"top\"],mask.index[mask]]=\"\"\n",
        "    #TODO: scientific notation - could be nicer\n",
        "    \n",
        "    return full_description"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HKh4hKT5_sG1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "link_data_solditems = \"https://drive.google.com/open?id=1Fi0Rkloif2hC73FelAMwKOJRDely7EG4\"\n",
        "fluff_data_solditems, id_data_solditems = link_data_solditems.split('=')\n",
        "\n",
        "downloaded_data_solditems = drive.CreateFile({'id':id_data_solditems}) \n",
        "downloaded_data_solditems.GetContentFile('solditems_encoded_stage2.csv')  \n",
        "df_data_solditems = pd.read_csv('solditems_encoded_stage2.csv')\n",
        "\n",
        "#print(df_data_solditems.head())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xNGR719-AfZo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "link_data_content = \"https://drive.google.com/open?id=1PgSjV_xgycDd2CDdBAV_maSfJesiQJgH\"\n",
        "fluff_data_content, id_data_content = link_data_content.split('=')\n",
        "\n",
        "downloaded_data_content = drive.CreateFile({'id':id_data_content}) \n",
        "downloaded_data_content.GetContentFile('content_encoded_stage2.csv')  \n",
        "df_data_content = pd.read_csv('content_encoded_stage2.csv')\n",
        "\n",
        "#print(df_data_content.head(1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zZ-GDUpGR6uZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = df_data_solditems.merge(df_data_content, left_on='product_sid', right_on='ProductId')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VtJt4alS0kTm",
        "colab_type": "code",
        "outputId": "81777a00-f741-4312-95b1-3d7e8fde6182",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(len(df_data_solditems),len(df_data_content))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "547248 3656\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SUsEtGbOmMy4",
        "colab_type": "code",
        "outputId": "3f9b61a7-d187-45c8-d9d4-d39d78111ad4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 386
        }
      },
      "source": [
        "for col in list(df_data_solditems):\n",
        "  print('items:',col)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "items: product_sid\n",
            "items: created_date_id\n",
            "items: sales_item_price_created\n",
            "items: sales_item_price\n",
            "items: sales_voucher_created\n",
            "items: sales_voucher\n",
            "items: sales_value_created\n",
            "items: sales_value\n",
            "items: created_year\n",
            "items: created_month\n",
            "items: created_weekday\n",
            "items: days_since_first_sold\n",
            "items: days_since_release\n",
            "items: Ratio_SalesItemPrice_per_created\n",
            "items: Ratio_SalesValue_per_created\n",
            "items: Ratio_SalesItemPrice_per_SalesValue\n",
            "items: Ratio_SalesItemPriceCreated_per_SalesValueCreated\n",
            "items: channel_sid_0\n",
            "items: channel_sid_1\n",
            "items: channel_sid_2\n",
            "items: returned_date_id_0\n",
            "items: returned_date_id_1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o7Y0tZMTxtOE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# These are manufacturers where a lot of data is available, which, again, reduces noise and makes prediction easier.\n",
        "# df_data_content.drop(columns=['Manufacturer_-1', 'Manufacturer_1', 'Manufacturer_2', 'Manufacturer_4', 'Manufacturer_5',\n",
        "#                  'Manufacturer_6', 'Manufacturer_7', 'Manufacturer_8', 'Manufacturer_9', 'Manufacturer_10',\n",
        "#                  'Manufacturer_11', 'Manufacturer_12', 'Manufacturer_13', 'Manufacturer_15', 'Manufacturer_16',\n",
        "#                  'Manufacturer_17', 'Manufacturer_18', 'Manufacturer_19', 'Manufacturer_20', 'Manufacturer_21',\n",
        "#                  'Manufacturer_22', 'Manufacturer_23', 'Manufacturer_24', 'Manufacturer_25', 'Manufacturer_26',\n",
        "#                  'Manufacturer_27', 'Manufacturer_28', 'Manufacturer_29', 'Manufacturer_30', 'Manufacturer_31',\n",
        "#                  'Manufacturer_33', 'Manufacturer_34', 'Manufacturer_35', 'Manufacturer_36', 'Manufacturer_37',\n",
        "#                  'Manufacturer_38', 'Manufacturer_39', 'Manufacturer_40', 'Manufacturer_41', 'Manufacturer_42',\n",
        "#                  'Manufacturer_43', 'Manufacturer_44', 'Manufacturer_45', 'Manufacturer_46', 'Manufacturer_47',\n",
        "#                  'Manufacturer_48', 'Manufacturer_49', 'Manufacturer_50', 'Manufacturer_51', 'Manufacturer_52',\n",
        "#                  'Manufacturer_53', 'Manufacturer_54', 'Manufacturer_55', 'Manufacturer_56', 'Manufacturer_57',\n",
        "#                  'Manufacturer_58', 'Manufacturer_59', 'Manufacturer_60', 'Manufacturer_61', 'Manufacturer_62',\n",
        "#                  'Manufacturer_63', 'Manufacturer_64', 'Manufacturer_65', 'Manufacturer_66', 'Manufacturer_67',\n",
        "#                  'Manufacturer_68', 'Manufacturer_69', 'Manufacturer_70', 'Manufacturer_71', 'Manufacturer_72',\n",
        "#                  'Manufacturer_73', 'Manufacturer_74'], \n",
        "#         inplace=True)\n",
        "\n",
        "# for col2 in list(df_data_content):\n",
        "#     print('content',col2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MLJClBBwxB3I",
        "colab_type": "code",
        "outputId": "305ba5dc-6b5b-46b9-bfc7-e8ea132a6a9c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        }
      },
      "source": [
        "df_data_content['ProductId'][:10]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    0\n",
              "1    1\n",
              "2    2\n",
              "3    3\n",
              "4    4\n",
              "5    5\n",
              "6    6\n",
              "7    7\n",
              "8    8\n",
              "9    9\n",
              "Name: ProductId, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UOcRR0AAQ97c",
        "colab_type": "code",
        "outputId": "c6bf2626-d939-4d6f-f185-d2c42ed3865f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(df_data_content)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3656"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vCrlLv-LQZnk",
        "colab_type": "code",
        "outputId": "a90394a3-033c-4f33-9999-48980a29bc42",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1058
        }
      },
      "source": [
        "df_data_content.apply(lambda x: x.count(), axis=1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0       848\n",
              "1       848\n",
              "2       848\n",
              "3       848\n",
              "4       848\n",
              "5       848\n",
              "6       848\n",
              "7       848\n",
              "8       848\n",
              "9       848\n",
              "10      848\n",
              "11      848\n",
              "12      848\n",
              "13      848\n",
              "14      848\n",
              "15      848\n",
              "16      848\n",
              "17      848\n",
              "18      848\n",
              "19      848\n",
              "20      848\n",
              "21      848\n",
              "22      848\n",
              "23      848\n",
              "24      848\n",
              "25      848\n",
              "26      848\n",
              "27      848\n",
              "28      848\n",
              "29      848\n",
              "       ... \n",
              "3626    848\n",
              "3627    848\n",
              "3628    848\n",
              "3629    848\n",
              "3630    848\n",
              "3631    848\n",
              "3632    848\n",
              "3633    848\n",
              "3634    848\n",
              "3635    848\n",
              "3636    848\n",
              "3637    848\n",
              "3638    848\n",
              "3639    848\n",
              "3640    848\n",
              "3641    848\n",
              "3642    848\n",
              "3643    848\n",
              "3644    848\n",
              "3645    848\n",
              "3646    848\n",
              "3647    848\n",
              "3648    848\n",
              "3649    848\n",
              "3650    848\n",
              "3651    848\n",
              "3652    848\n",
              "3653    848\n",
              "3654    848\n",
              "3655    848\n",
              "Length: 3656, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "92UlcAuGNKpF",
        "colab_type": "code",
        "outputId": "c0cacef2-83fa-4485-b8dd-483893ddc3b9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "data_new = data_new.dropna()\n",
        "len(data_new)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1377"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MHpVzBx2K7CA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "data_new = df_data_content[['Manufacturer_-1', 'Manufacturer_1', 'Manufacturer_2', 'Manufacturer_4', 'Manufacturer_5',\n",
        "                 'Manufacturer_6', 'Manufacturer_7', 'Manufacturer_8', 'Manufacturer_9', 'Manufacturer_10',\n",
        "                 'Manufacturer_11', 'Manufacturer_12', 'Manufacturer_13', 'Manufacturer_15', 'Manufacturer_16',\n",
        "                 'Manufacturer_17', 'Manufacturer_18', 'Manufacturer_19', 'Manufacturer_20', 'Manufacturer_21',\n",
        "                 'Manufacturer_22', 'Manufacturer_23', 'Manufacturer_24', 'Manufacturer_25', 'Manufacturer_26',\n",
        "                 'Manufacturer_27', 'Manufacturer_28', 'Manufacturer_29', 'Manufacturer_30', 'Manufacturer_31',\n",
        "                 'Manufacturer_33', 'Manufacturer_34', 'Manufacturer_35', 'Manufacturer_36', 'Manufacturer_37',\n",
        "                 'Manufacturer_38', 'Manufacturer_39', 'Manufacturer_40', 'Manufacturer_41', 'Manufacturer_42',\n",
        "                 'Manufacturer_43', 'Manufacturer_44', 'Manufacturer_45', 'Manufacturer_46', 'Manufacturer_47',\n",
        "                 'Manufacturer_48', 'Manufacturer_49', 'Manufacturer_50', 'Manufacturer_51', 'Manufacturer_52',\n",
        "                 'Manufacturer_53', 'Manufacturer_54', 'Manufacturer_55', 'Manufacturer_56', 'Manufacturer_57',\n",
        "                 'Manufacturer_58', 'Manufacturer_59', 'Manufacturer_60', 'Manufacturer_61', 'Manufacturer_62',\n",
        "                 'Manufacturer_63', 'Manufacturer_64', 'Manufacturer_65', 'Manufacturer_66', 'Manufacturer_67',\n",
        "                 'Manufacturer_68', 'Manufacturer_69', 'Manufacturer_70', 'Manufacturer_71', 'Manufacturer_72',\n",
        "                 'Manufacturer_73', 'Manufacturer_74']].replace(1,np.nan)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tye_TcFaBLN8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = df_data_solditems.merge(df_data_content, left_on='product_sid', right_on='ProductId')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ho2srtPBD24o",
        "colab_type": "code",
        "outputId": "f27298d9-db70-404d-91e0-29d03e583e3a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        }
      },
      "source": [
        "print(df.columns)\n",
        "print(len(df.columns))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Index(['product_sid', 'created_date_id', 'sales_item_price_created',\n",
            "       'sales_item_price', 'sales_voucher_created', 'sales_voucher',\n",
            "       'sales_value_created', 'sales_value', 'created_year', 'created_month',\n",
            "       ...\n",
            "       'OS_5', 'OS_6', 'OS_7', 'OS_8', 'Radio_0', 'Radio_1', 'UMTS_0',\n",
            "       'UMTS_1', 'WLAN_0', 'WLAN_1'],\n",
            "      dtype='object', length=870)\n",
            "870\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-02T15:25:00.835900Z",
          "start_time": "2019-05-02T15:25:00.667466Z"
        },
        "id": "R9n4TVwKaxvr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# There is a warning that would be worth investigationg, but for now, let's ignore it\n",
        "# import warnings\n",
        "# warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-02T15:25:00.992198Z",
          "start_time": "2019-05-02T15:25:00.847885Z"
        },
        "id": "H9epdfIvaxvy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# df[\"date\"]= pd.to_datetime(df['year'].astype(str)+'-'+df['month'].astype(str)+\"-\"+df[\"day\"].astype(str)+\"T\"+df[\"hour\"].astype(str).apply(lambda x: x.zfill(2)+\":00\"))\n",
        "# df.set_index(df.date, inplace=True)\n",
        "# df.drop(\"date\", axis=1, inplace=True)\n",
        "# df.head(10)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BzQvMRutKKHc",
        "colab_type": "text"
      },
      "source": [
        "## Exploratory Data Analysis (EDA)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZOREJvmFKRn5",
        "colab_type": "code",
        "outputId": "db3acf65-c203-4ae1-e36f-7274e2863fc3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(len(df.columns))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "870\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GYFc5Fb3axv1",
        "colab_type": "text"
      },
      "source": [
        "## Encoding day of week\n",
        "\n",
        "We explicitly encode the day of week, since we assume that weekends and workdays behave differently."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-02T15:25:01.000339Z",
          "start_time": "2019-05-02T15:25:00.993439Z"
        },
        "id": "9txti2UVaxv2",
        "colab_type": "code",
        "outputId": "84025e57-f993-4bef-83f8-6052fab84599",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        }
      },
      "source": [
        "df[\"dayofweek\"]=df.index.dayofweek+1\n",
        "# Starts with 1 (Monday); usually, Monday is 0. Only 1-7 is in the column."
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-49-63444aaaf3cc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"dayofweek\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdayofweek\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# Starts with 1 (Monday); usually, Monday is 0. Only 1-7 is in the column.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'Int64Index' object has no attribute 'dayofweek'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3jV-wvEZaxv4",
        "colab_type": "text"
      },
      "source": [
        "# Decision about NaN-s"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-02T15:25:01.354703Z",
          "start_time": "2019-05-02T15:25:01.001819Z"
        },
        "id": "KdOXxi_Saxv5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig = plt.gcf()\n",
        "fig.set_size_inches(10,7)\n",
        "df[\"pm2.5\"].isnull().astype(float).plot()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-02T15:25:01.432487Z",
          "start_time": "2019-05-02T15:25:01.356144Z"
        },
        "id": "iMBFjck4axv8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#df.groupby(df.index.weekday_name)[\"pm2.5\"].isnull().astype(int).sum()\n",
        "def isnullsum(df):\n",
        "    return df.isnull().sum()\n",
        "\n",
        "\n",
        "#print(\"All datapoints per year:\",df.groupby(df.index.year)[\"pm2.5\"].count())\n",
        "#print(\"NaN datapoints per year:\",df.groupby(df.index.year)[\"pm2.5\"].apply(isnullsum))\n",
        "print(\"% NaN datapoints per year:\",(df.groupby(df.index.year)[\"pm2.5\"].apply(isnullsum)/df.groupby(df.index.year)[\"pm2.5\"].count())*100.0)\n",
        "\n",
        "#print(\"--------------------------------\")\n",
        "\n",
        "\n",
        "#print(\"All datapoints per month:\",df.groupby(df.index.month)[\"pm2.5\"].count())\n",
        "#print(\"NaN datapoints per month:\",df.groupby(df.index.month)[\"pm2.5\"].apply(isnullsum))\n",
        "print(\"% NaN datapoints per year:\",(df.groupby(df.index.month)[\"pm2.5\"].apply(isnullsum)/df.groupby(df.index.month)[\"pm2.5\"].count())*100.0)\n",
        "\n",
        "#print(\"--------------------------------\")\n",
        "\n",
        "\n",
        "#print(\"All datapoints per weekday:\",df.groupby(df.index.weekday_name)[\"pm2.5\"].count())\n",
        "#print(\"NaN datapoints per weekday:\",df.groupby(df.index.weekday_name)[\"pm2.5\"].apply(isnullsum))\n",
        "print(\"% NaN datapoints per year:\",(df.groupby(df.index.weekday_name)[\"pm2.5\"].apply(isnullsum)/df.groupby(df.index.weekday_name)[\"pm2.5\"].count())*100.0)\n",
        "\n",
        "#print(\"--------------------------------\")\n",
        "\n",
        "#print(\"All datapoints per weekday:\",df.groupby(df.index.hour)[\"pm2.5\"].count())\n",
        "#print(\"NaN datapoints per weekday:\",df.groupby(df.index.hour)[\"pm2.5\"].apply(isnullsum))\n",
        "print(\"% NaN datapoints per year:\",(df.groupby(df.index.hour)[\"pm2.5\"].apply(isnullsum)/df.groupby(df.index.hour)[\"pm2.5\"].count())*100.0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sI8vSQHaaxv_",
        "colab_type": "text"
      },
      "source": [
        "After examining the NaN values in pm2.5, we see no obvious temporal pattern. This is cause for worry, since by simply dropping the rows with NaN values, we can destroy the temporal coherence of the data, hence **data imputation is desirable.**\n",
        "\n",
        "The autocorrelation charts below imply, that it is not unreasonable to take the previous value to fill NaN-s (high autocorrelation with the previous timestep)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-02T15:25:01.471480Z",
          "start_time": "2019-05-02T15:25:01.433895Z"
        },
        "id": "hSjYGbzkaxv_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.fillna(method='ffill', inplace=True)\n",
        "\n",
        "print(df.isnull().sum())\n",
        "\n",
        "df.dropna(inplace=True)\n",
        "\n",
        "print(df.isnull().sum())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eqcSD45HaxwD",
        "colab_type": "text"
      },
      "source": [
        "# Examining autocorrelations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-02T15:25:25.702761Z",
          "start_time": "2019-05-02T15:25:01.472908Z"
        },
        "id": "Hq3jB2m2axwD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from statsmodels.graphics.tsaplots import plot_pacf\n",
        "\n",
        "#columns = [] #use this for speedup\n",
        "columns = [\"pm2.5\",\"DEWP\",\"TEMP\",\"PRES\",\"Iws\",\"Is\",\"Ir\"]\n",
        "\n",
        "for col in columns:\n",
        "    plt.figure()\n",
        "    plot_pacf(df[col].dropna(), lags=200, zero=False)\n",
        "    \n",
        "plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k8SFJ4RQaxwH",
        "colab_type": "text"
      },
      "source": [
        "## What do we see?\n",
        "\n",
        "Well, the fact, that we don't see.\n",
        "\n",
        "Or more precisely: smog (and weather) is slow to move, it is extremely strongly autocorrelated with itself one-two hours before, so in order to at least be able to see some autocorrelation structure beyond this, we need to filter out the first some hours from our autocorrelation analysis.\n",
        "\n",
        "(By the way, that's why we don't stick to the prediction of the next hour as in the original \"inspiration\" blogpost. Would not be too relevant...)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-02T15:25:25.715038Z",
          "start_time": "2019-05-02T15:25:25.705295Z"
        },
        "id": "vR99efUZaxwI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from statsmodels.graphics.tsaplots import _prepare_data_corr_plot, _plot_corr\n",
        "import statsmodels.graphics.utils as utils\n",
        "from statsmodels.tsa.stattools import pacf\n",
        "\n",
        "def plot_pacf_drop(x, ax=None, lags=None, alpha=.05, method='ywunbiased',\n",
        "              use_vlines=True, title='Partial Autocorrelation', zero=True,\n",
        "              vlines_kwargs=None, drop_no=0, **kwargs):\n",
        "    \n",
        "    lags_orig=lags\n",
        "    fig, ax = utils.create_mpl_ax(ax)\n",
        "    vlines_kwargs = {} if vlines_kwargs is None else vlines_kwargs\n",
        "    lags, nlags, irregular = _prepare_data_corr_plot(x, lags, zero)\n",
        "    confint = None\n",
        "    if alpha is None:\n",
        "        acf_x = pacf(x, nlags=nlags, alpha=alpha, method=method)\n",
        "    else:\n",
        "        acf_x, confint = pacf(x, nlags=nlags, alpha=alpha, method=method)\n",
        "\n",
        "    if drop_no:\n",
        "        acf_x = acf_x[drop_no+1:]\n",
        "        confint = confint[drop_no+1:]\n",
        "        lags, nlags, irregular = _prepare_data_corr_plot(x, lags_orig-drop_no, zero)\n",
        "        \n",
        "    _plot_corr(ax, title, acf_x, confint, lags, False, use_vlines,\n",
        "               vlines_kwargs, **kwargs)\n",
        "\n",
        "    return fig"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-02T15:25:56.406955Z",
          "start_time": "2019-05-02T15:25:25.718313Z"
        },
        "scrolled": false,
        "id": "l1r3u16jaxwK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#columns = [] #use this for speedup\n",
        "columns = [\"pm2.5\",\"DEWP\",\"TEMP\",\"PRES\",\"Iws\",\"Is\",\"Ir\"]\n",
        "\n",
        "for col in columns:\n",
        "    plt.figure()\n",
        "    plot_pacf_drop(df[col].dropna(), lags=200, drop_no=3, zero=False)\n",
        "    \n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WxeKanB5axwN",
        "colab_type": "text"
      },
      "source": [
        "Studying even the filtered charts leaves us in doubt about the possible window for modeling (in case of the classical models), so we will keep 200 as the modeling window (nearly two weeks). This is a parameter that is worth empirically studying later on.\n",
        "\n",
        "It is worth mentioning, that the `pacf` charts would definitely change drastically if we would use some differencing. Since down below we decide not to, we keep it as it is."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R-V63IQJaxwO",
        "colab_type": "text"
      },
      "source": [
        "## Seasonal decomposition and the question of trends"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-02T15:25:57.851934Z",
          "start_time": "2019-05-02T15:25:56.409704Z"
        },
        "id": "nqR3HKvvaxwP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "from statsmodels.tsa.tsatools import freq_to_period\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "analysis = seasonal_decompose(df[\"pm2.5\"], freq=freq_to_period(df.index.inferred_freq))\n",
        "\n",
        "analysis.plot()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tfOXwWKfaxwT",
        "colab_type": "text"
      },
      "source": [
        "Well, the default setting (infer periods - hourly) is rather uninformative, so it is maybe worth using some domain knowledge here, and use yearly frequency."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-02T15:25:59.931549Z",
          "start_time": "2019-05-02T15:25:57.854582Z"
        },
        "id": "IrFr4ytZaxwU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "analysis = seasonal_decompose(df[\"pm2.5\"], freq=24*365)\n",
        "\n",
        "analysis.plot()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HLb46sazaxwb",
        "colab_type": "text"
      },
      "source": [
        "We do get the first impression, that there is no overarching simple trend, as well as there are non-trivial seasonal patterns. At a later stage we should investigate differencing regimes, but for now, we leave the data as is."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LecB6oWOaxwc",
        "colab_type": "text"
      },
      "source": [
        "# Train, valid, test split - before normalization\n",
        "\n",
        "Contamination by the normalization values is a distant possibility, but let's stick to paranoid practices."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-02T15:25:59.936881Z",
          "start_time": "2019-05-02T15:25:59.933655Z"
        },
        "id": "CIAbPCdpaxwe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "VALID_AND_TEST_SIZE=0.1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-02T15:26:00.056548Z",
          "start_time": "2019-05-02T15:25:59.940082Z"
        },
        "id": "zVOPIcqLaxwh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_else, y_train, y_else = train_test_split(df, df[\"pm2.5\"], test_size=VALID_AND_TEST_SIZE*2, shuffle=False)\n",
        "X_valid, X_test, y_valid, y_test = train_test_split(X_else, y_else, test_size=0.5, shuffle=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "smPhgV7Daxwk",
        "colab_type": "text"
      },
      "source": [
        "We could have used `temporal_split` from `seglearn`, but that would have cast everything to numpy, so it was more convenient this way for now. Using `seglearn` is encouraged - if we would like to go into classical modeling."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-04-28T08:49:44.040034Z",
          "start_time": "2019-04-28T08:49:43.994879Z"
        },
        "id": "cJAiMjoTaxwl",
        "colab_type": "text"
      },
      "source": [
        "# Data normalization\n",
        "\n",
        "Our default assumption is to use Scikit's minmax scaler for easier learning by neural models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-04-28T08:49:44.040034Z",
          "start_time": "2019-04-28T08:49:43.994879Z"
        },
        "id": "b5MA3Qwraxwm",
        "colab_type": "text"
      },
      "source": [
        "But there are some exceptions:\n",
        "\n",
        "## How to normalize dates?\n",
        "\n",
        "For the year it is more tricky, it is basically an ordinal.\n",
        "Subtracting the first year is nice, but how to handle the normalization to 0,1?\n",
        "\n",
        "We could use 2018 as a max, but **WE WOULD HAVE TO WRITE A BIG CAVEAT MESSAGE FOR DEPLOY PEOPLE!**\n",
        "\n",
        "So it should be something like  `(df.year - (df.year.min())-1)/((df.year.max()-df.year.min())*2)` (-1 is for avoiding zero, making the life of the network more easy...)\n",
        "\n",
        "For now we stick to the minmax scaler (living risky... :-)\n",
        "\n",
        "For month, day, hour default assumption is, scikit's minmax scaler could work, but we will choose a more elaborate solution from [here](https://ianlondon.github.io/blog/encoding-cyclical-features-24hour-time/). This capitalizes on the circular nature of these quasi ordinals.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-02T15:26:00.065112Z",
          "start_time": "2019-05-02T15:26:00.059198Z"
        },
        "id": "_sO0Uphvaxwn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import warnings\n",
        "from sklearn.exceptions import DataConversionWarning\n",
        "warnings.filterwarnings(action='ignore', category=DataConversionWarning)\n",
        "# I literally hate when a standard Scikit function throws big bunches of warnings \n",
        "# - though suppressing them is a dangerous practice. Hence this comment. \n",
        "\n",
        "def minmax_scale(df_x,series_y, normalizers=None):\n",
        "    features_to_minmax = [\"year\",\"pm2.5\",\"DEWP\",\"TEMP\",\"PRES\",\"Iws\",\"Is\",\"Ir\"]\n",
        "\n",
        "    if not normalizers:\n",
        "        normalizers = {}\n",
        "\n",
        "    for feat in features_to_minmax:\n",
        "        if feat not in normalizers:\n",
        "            normalizers[feat] = MinMaxScaler()\n",
        "            normalizers[feat].fit(df_x[feat].values.reshape(-1, 1))\n",
        "        \n",
        "        df_x[feat] = normalizers[feat].transform(df_x[feat].values.reshape(-1, 1))\n",
        "\n",
        "    series_y=normalizers[\"pm2.5\"].transform(series_y.values.reshape(-1, 1))\n",
        "\n",
        "    return df_x, series_y, normalizers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-02T15:26:00.964633Z",
          "start_time": "2019-05-02T15:26:00.067037Z"
        },
        "id": "6AAFsnMIaxwp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train_norm, y_train_norm, normalizers = minmax_scale(X_train, y_train)\n",
        "X_valid_norm, y_valid_norm, _ = minmax_scale(X_valid, y_valid, normalizers=normalizers)\n",
        "X_test_norm, y_test_norm, _ = minmax_scale(X_test, y_test, normalizers=normalizers)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-04-28T08:49:44.040034Z",
          "start_time": "2019-04-28T08:49:43.994879Z"
        },
        "id": "3Vi-RZ1faxwr",
        "colab_type": "text"
      },
      "source": [
        "## Encoding of ordinals\n",
        "\n",
        "The encoding of `cbwd` is interesting, since it is an ordinal again, or better to say not even that, it has a nice circular topology, so we will use the same sin-cos solution.\n",
        "\n",
        "Problem is, that there is a valid \"zero\" value, marked \"cv\" in there. We are tempted to replace that with 0."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-02T15:26:00.974581Z",
          "start_time": "2019-05-02T15:26:00.966574Z"
        },
        "id": "5tCsbp1baxwt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def encode_cyclicals(df_x):\n",
        "    #\"month\",\"day\",\"hour\", \"cdbw\", \"dayofweek\"\n",
        "    \n",
        "    DIRECTIONS = {\"N\":1.0,\"NE\":2.0, \"E\":3.0, \"SE\":4.0, \"S\":5.0, \"SW\":6.0, \"W\":7.0, \"NW\":8.0, \"cv\":np.nan}\n",
        "\n",
        "    df_x['month_sin'] = np.sin(2*np.pi*df_x.month/12)\n",
        "    df_x['month_cos'] = np.cos(2*np.pi*df_x.month/12)\n",
        "    df_x.drop('month', axis=1, inplace=True)\n",
        "    \n",
        "    df_x['day_sin'] = np.sin(2*np.pi*df_x.day/31)\n",
        "    df_x['day_cos'] = np.cos(2*np.pi*df_x.day/31)\n",
        "    df_x.drop('day', axis=1, inplace=True)\n",
        "\n",
        "    df_x['dayofweek_sin'] = np.sin(2*np.pi*df_x.dayofweek/7)\n",
        "    df_x['dayofweek_cos'] = np.cos(2*np.pi*df_x.dayofweek/7)\n",
        "    df_x.drop('dayofweek', axis=1, inplace=True)\n",
        "    \n",
        "    df_x['hour_sin'] = np.sin(2*np.pi*df_x.hour/24)\n",
        "    df_x['hour_cos'] = np.cos(2*np.pi*df_x.hour/24)\n",
        "    df_x.drop('hour', axis=1, inplace=True)\n",
        "    \n",
        "    df_x.replace({'cbwd': DIRECTIONS}, inplace=True)\n",
        "    df_x['cbwd'] = df_x['cbwd'].astype(np.float64) \n",
        "\n",
        "    df_x['cbwd_sin'] = np.sin(2.0*np.pi*df_x.cbwd/8.0)\n",
        "    df_x['cbwd_sin'].replace(np.nan, 0.0, inplace=True) #Let's handle the case with no wind specially\n",
        "    df_x['cbwd_cos'] = np.cos(2.0*np.pi*df_x.cbwd/8.0)\n",
        "    df_x['cbwd_cos'].replace(np.nan, 0.0, inplace=True) #Let's handle the case with no wind specially\n",
        "    df_x.drop('cbwd', axis=1, inplace=True)\n",
        "    \n",
        "    return df_x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-02T15:26:03.103910Z",
          "start_time": "2019-05-02T15:26:00.977468Z"
        },
        "id": "_Yr0nFtKaxwv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train_norm = encode_cyclicals(X_train_norm)\n",
        "X_valid_norm = encode_cyclicals(X_valid_norm)\n",
        "X_test_norm = encode_cyclicals(X_test_norm)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-02T15:26:03.157729Z",
          "start_time": "2019-05-02T15:26:03.105840Z"
        },
        "id": "i8Cnwo0yaxwy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train_norm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-02T15:26:03.164598Z",
          "start_time": "2019-05-02T15:26:03.159269Z"
        },
        "id": "fyhNggVAaxw2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Just in case to ensure we did everything right\n",
        "assert all(x==np.float64 for x in list(X_train_norm.dtypes))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YDk2huLLaxw6",
        "colab_type": "text"
      },
      "source": [
        "It would be worth checking with some assertions that the manual normalizers work well. Let's leave it to later work."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uZEdEZDaxw6",
        "colab_type": "text"
      },
      "source": [
        "It is also worth noting, that the normalizers should be saved and used in production."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XQRIeDORaxw7",
        "colab_type": "text"
      },
      "source": [
        "# Creating target (y) and \"windows\" (X) for modeling\n",
        "\n",
        "By default we use the next 24 hour value of \"pm2.5\" for prediction, that is, I would like to predict what the pm2.5 will be like **at this hour 24 hours from now.**\n",
        "\n",
        "We use the quite handy **seglearn** package for this."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7qFfklmaxw9",
        "colab_type": "text"
      },
      "source": [
        "Because of computational reasons, we **use the window of 100 hours** to predict. Classical models would have hard time to accommodate substantially (like 5-10x) context windows, LSTM-s would suffer from the challenge of long term memory. After a basic run of modeling the next big challenge would be to investigate PACF structure more and use eg. stateful LSTM modeling to try to accommodate the large \"lookback\".   "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-02T15:26:03.169133Z",
          "start_time": "2019-05-02T15:26:03.165982Z"
        },
        "id": "XQggSfX2axw-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TIME_WINDOW=100\n",
        "FORECAST_DISTANCE=24"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-02T15:26:05.135398Z",
          "start_time": "2019-05-02T15:26:03.170483Z"
        },
        "id": "5XC0DRLIaxxC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from seglearn.transform import FeatureRep, SegmentXYForecast, last\n",
        "\n",
        "segmenter = SegmentXYForecast(width=TIME_WINDOW, step=1, y_func=last, forecast=FORECAST_DISTANCE)\n",
        "\n",
        "X_train_rolled, y_train_rolled,_=segmenter.fit_transform([X_train_norm.values],[y_train_norm.flatten()])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-02T15:26:05.141218Z",
          "start_time": "2019-05-02T15:26:05.137116Z"
        },
        "id": "Qv6DWUMMaxxF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train_rolled[:1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t84EiP1EaxxJ",
        "colab_type": "text"
      },
      "source": [
        "# For non-sequence models \n",
        "\n",
        "We have to \"flatten\" the data to be able to use classical, non-sequence regression models from Scikit.\n",
        "\n",
        "**We only need to do this for X, any transformation of y is unnecessary.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-02T15:26:05.154555Z",
          "start_time": "2019-05-02T15:26:05.142809Z"
        },
        "id": "L4fEI7jNaxxK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train_rolled.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-02T15:26:05.161443Z",
          "start_time": "2019-05-02T15:26:05.156594Z"
        },
        "id": "7CbFw4cwaxxM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "shape = X_train_rolled.shape\n",
        "X_train_flattened = X_train_rolled.reshape(shape[0],shape[1]*shape[2])\n",
        "X_train_flattened.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-02T15:26:05.405298Z",
          "start_time": "2019-05-02T15:26:05.163304Z"
        },
        "id": "QlT9TcesaxxO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_valid_rolled, y_valid_rolled,_=segmenter.fit_transform([X_valid_norm.values],[y_valid_norm.flatten()])\n",
        "\n",
        "shape = X_valid_rolled.shape\n",
        "X_valid_flattened = X_valid_rolled.reshape(shape[0],shape[1]*shape[2])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hBIMzSQ8axxQ",
        "colab_type": "text"
      },
      "source": [
        "# Evaluation helper\n",
        "\n",
        "Use this function to evaluate your models **on validation data.**\n",
        "\n",
        "This assumes that your model has the `predict()` function, which is true for **Scikit-learn, XGBoost** and **Keras**, so you can can hand over any of those. \n",
        "\n",
        "A special issue by models optimized by iterative methods is to **get the final model**. **Early stopping and / or model save and reload** can help there.  \n",
        "\n",
        "**WARNING: This is just a basic evaluation scheme, more thorough investigation needed in the future!**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-02T15:26:05.414032Z",
          "start_time": "2019-05-02T15:26:05.407874Z"
        },
        "id": "0Juf0tmtaxxR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "from math import sqrt\n",
        "\n",
        "def evaluate_model(model, X_valid, y_valid_true):\n",
        "    predictions = model.predict(X_valid)\n",
        "    rms = sqrt(mean_squared_error(y_valid_true, predictions))\n",
        "    print(\"Root mean squared error on valid:\",rms)\n",
        "    normalized_rms = normalizers[\"pm2.5\"].inverse_transform(np.array([rms]).reshape(1, -1))[0][0]\n",
        "    print(\"Root mean squared error on valid inverse transformed from normalization:\",normalized_rms)\n",
        "    return normalized_rms"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fqjItXrRaxxT",
        "colab_type": "text"
      },
      "source": [
        "# Classical modeling\n",
        "\n",
        "In \"classical\" modeling we assume a multiple regression case, so we **DO NOT USE time series as such, but the \"flat\" versions of the data** as input. Output is the same. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v_zm3ZKNaxxU",
        "colab_type": "text"
      },
      "source": [
        "## Baseline - DummyPredictor\n",
        "\n",
        "**TASK Create a dummy predictor as a baseline. Use Scikit-learn's builtin capability to do dummy models in regression case. Use the default setting, that is the prediction of the mean value.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-02T15:26:05.425330Z",
          "start_time": "2019-05-02T15:26:05.416635Z"
        },
        "id": "lX_aHWA_axxV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.dummy import DummyRegressor\n",
        "\n",
        "dummy_model = DummyRegressor(strategy='mean', constant=None, quantile=None)\n",
        "\n",
        "dummy_model.fit(X_train_flattened, y_train_rolled)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YU3FMbsjaxxY",
        "colab_type": "text"
      },
      "source": [
        "### Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-02T15:26:05.433386Z",
          "start_time": "2019-05-02T15:26:05.428185Z"
        },
        "id": "B5f02jBeaxxY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "result = evaluate_model(dummy_model, X_valid_flattened, y_valid_rolled)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8UgotCGKaxxb",
        "colab_type": "text"
      },
      "source": [
        "## Fitting a RandomForest on raw data\n",
        "\n",
        "**TASK: Fit a RandomForest from Scikit. Please be aware, that the number of trees in the model is having a strong influence on training time.** \n",
        "\n",
        "**Suggestion:** use couple of tens of trees, definitely << 100 to be able to wait it out...\n",
        "\n",
        "**Pro tip:** To utilize all the CPU cores, use the right setting of n_jobs. That speeds things up."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-02T15:26:05.455678Z",
          "start_time": "2019-05-02T15:26:05.436188Z"
        },
        "id": "Vzd24Epuaxxg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "N_ESTIMATORS = 10\n",
        "RANDOM_STATE = 452543634"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-02T15:31:10.504738Z",
          "start_time": "2019-05-02T15:26:05.457739Z"
        },
        "id": "AEKbzRcRaxxk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "RF_base_model = RandomForestRegressor(n_estimators=N_ESTIMATORS, \n",
        "                                      criterion='mse', \n",
        "                                      max_depth=None, \n",
        "                                      min_samples_split=2, \n",
        "                                      min_samples_leaf=1, \n",
        "                                      min_weight_fraction_leaf=0.0, \n",
        "                                      max_features='auto', \n",
        "                                      max_leaf_nodes=None, \n",
        "                                      min_impurity_decrease=0.0, \n",
        "                                      min_impurity_split=None, \n",
        "                                      bootstrap=True, \n",
        "                                      oob_score=False, \n",
        "                                      n_jobs=None, \n",
        "                                      random_state=RANDOM_STATE, \n",
        "                                      verbose=0, \n",
        "                                      warm_start=False)\n",
        "\n",
        "RF_base_model.fit(X_train_flattened, y_train_rolled)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dd6GYucSaxxn",
        "colab_type": "text"
      },
      "source": [
        "### Evaluation\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-02T15:31:10.647413Z",
          "start_time": "2019-05-02T15:31:10.507882Z"
        },
        "id": "gRh74bFsaxxo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "result = evaluate_model(RF_base_model, X_valid_flattened, y_valid_rolled)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zTqqUohjaxxu",
        "colab_type": "text"
      },
      "source": [
        "## Fitting a RandomForest on feature transformed data\n",
        "\n",
        "**TASK: Since we use `seglearn`, we can try to capitalize on it's functionality to calculate features from the time time series. Use `FeatureRep` from `seglearn` to transform features, fit a RandomForest and hope for the best!**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-02T15:32:02.592160Z",
          "start_time": "2019-05-02T15:31:10.654979Z"
        },
        "id": "XsgTPF5daxxv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# https://dmbee.github.io/seglearn/transform.html\n",
        "\n",
        "RF_feature_model = RandomForestRegressor(n_estimators=N_ESTIMATORS, \n",
        "                                      criterion='mse', \n",
        "                                      max_depth=None, \n",
        "                                      min_samples_split=2, \n",
        "                                      min_samples_leaf=1, \n",
        "                                      min_weight_fraction_leaf=0.0, \n",
        "                                      max_features='auto', \n",
        "                                      max_leaf_nodes=None, \n",
        "                                      min_impurity_decrease=0.0, \n",
        "                                      min_impurity_split=None, \n",
        "                                      bootstrap=True, \n",
        "                                      oob_score=False, \n",
        "                                      n_jobs=None, \n",
        "                                      random_state=RANDOM_STATE, \n",
        "                                      verbose=0, \n",
        "                                      warm_start=False)\n",
        "\n",
        "feature_converter = FeatureRep()\n",
        "\n",
        "RF_feature_model.fit(feature_converter.fit_transform(X_train_flattened), y_train_rolled)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "shBTQEdCaxxy",
        "colab_type": "text"
      },
      "source": [
        "### Evaluation\n",
        "\n",
        "WARNING: This is just a basic evaluation scheme, more thorough investigation needed!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-02T15:32:04.242528Z",
          "start_time": "2019-05-02T15:32:02.598939Z"
        },
        "id": "jL46RZgeaxxz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "result = evaluate_model(RF_feature_model,feature_converter.fit_transform(X_train_flattened), y_train_rolled)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1po9e8cFaxx3",
        "colab_type": "text"
      },
      "source": [
        "## XGBoost for speedup\n",
        "\n",
        "Use the XGBoost library to fit gradient boosted trees to the problem. They are usually way quicker to learn and many times at least on par with RandomForests, or better. Let's see!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-02T15:32:05.234439Z",
          "start_time": "2019-05-02T15:32:04.248920Z"
        },
        "id": "cxAjUB3Daxx3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import xgboost as xgb\n",
        "# If in trouble, use !pip install xgboost\n",
        "\n",
        "# XGBoost needs it's custom data format to run quickly\n",
        "dmatrix_train = xgb.DMatrix(data=X_train_flattened,label=y_train_rolled)\n",
        "dmatrix_valid = xgb.DMatrix(data=X_valid_flattened,label=y_valid_rolled)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-02T15:32:13.647736Z",
          "start_time": "2019-05-02T15:32:05.237071Z"
        },
        "id": "K39mw9ovaxx5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "params = {'objective': 'reg:linear', 'eval_metric': 'rmse', 'n_estimators': 3}\n",
        "\n",
        "evallist = [(dmatrix_valid, 'eval'), (dmatrix_train, 'train')]\n",
        "\n",
        "num_round = 3 #Can easily overfit, experiment with it!\n",
        "\n",
        "xg_reg = xgb.train(params, dmatrix_train, num_round, evallist, early_stopping_rounds=5)\n",
        "# https://stackoverflow.com/questions/37650892/xgboost-on-python-what-is-wrong-with-xgb-cv"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-02T15:32:13.654770Z",
          "start_time": "2019-05-02T15:32:13.649668Z"
        },
        "id": "VcR4M_N8axx8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "result = evaluate_model(xg_reg, dmatrix_valid, y_valid_rolled)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V5vUfUvCaxyA",
        "colab_type": "text"
      },
      "source": [
        "# Building an LSTM model\n",
        "\n",
        "## Modeling assumptions\n",
        "\n",
        "**TASK:** We believe, that the time dependent structure of this dataset is complex, so we try to use LSTM models from Keras. We are not explicitly utilizing **statefulness**, that is a **major area to be investigated later on**. We use GPU optimizations with the help of [CuDNNLSTM](https://www.tensorflow.org/api_docs/python/tf/keras/layers/CuDNNLSTM) for high speed training.\n",
        "\n",
        "More information on statefulness can be found [here](https://philipperemy.github.io/keras-stateful-lstm/).\n",
        "\n",
        "\n",
        "Fit an LSTM model **on the time series - non-flat - data!**.\n",
        "\n",
        "Use:\n",
        "1. At least 1 LSTM layer\n",
        "2. A dense layer for output - think about activation! This is a regression case!\n",
        "\n",
        "**Very advisable** - but optional - to use Dropout. Sadly the CuDNNLSTM implementation does not have it built in. You can choose: Either use normal LSTM (not really a good idea, just check the speed of learning!) or use CuDNNLSTM and a dropout layer. You can not use it everywhere, though... Experiment!\n",
        "\n",
        "You are allowed to use functional API, but for this **Sequential API is sufficient.**\n",
        "\n",
        "You **can use LeraningRateScheduler** if you like."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-02T15:36:43.004415Z",
          "start_time": "2019-05-02T15:36:43.001752Z"
        },
        "id": "V5zN6fF2axyA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "LSTM_CELL_SIZE = 5\n",
        "BATCH_SIZE = 500\n",
        "EPOCHS = 20\n",
        "DROPOUT_RATE=0.1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-02T15:43:36.343431Z",
          "start_time": "2019-05-02T15:36:43.007323Z"
        },
        "id": "qnD4h4CmaxyD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, CuDNNLSTM\n",
        "from tensorflow.keras import backend as be\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
        "from keras.models import load_model\n",
        "\n",
        "column_count = len(X_train.columns) #Remember,column count before rolling...\n",
        "\n",
        "be.clear_session()\n",
        "\n",
        "# You might very well be needing it!\n",
        "# Remeber to save only what is worth it from validation perspective...\n",
        "# model_saver = ModelCheckpoint(...)\n",
        "\n",
        "# If you need it...\n",
        "#def schedule(epoch, lr):\n",
        "#    ...\n",
        "#    return lr\n",
        "\n",
        "#lr_scheduler = LearningRateScheduler(schedule)\n",
        "\n",
        "# Build your whole LSTM model here!\n",
        "model = Sequential()\n",
        "\n",
        "model.add(CuDNNLSTM(units=LSTM_CELL_SIZE, input_shape=(TIME_WINDOW, column_count)))\n",
        "\n",
        "model.add(Dense(1, activation = 'linear'))\n",
        "\n",
        "model.compile(optimizer='adadelta', loss='mean_squared_error')\n",
        "\n",
        "#For shape remember, we have a variable defining the \"window\" and the features in the window...\n",
        "\n",
        "# Fit on the train data\n",
        "# USE the batch size parameter!\n",
        "# Use validation data - warning, a tuple of stuff!\n",
        "# Epochs as deemed necessary...\n",
        "# You should avoid shuffling the data maybe.\n",
        "# You can use the callbacks for LR schedule or model saving as seems fit.\n",
        "history = model.fit(X_train_rolled, y_train_rolled, epochs=EPOCHS, batch_size=BATCH_SIZE, validation_data=(X_valid_rolled, y_valid_rolled), class_weight=None, sample_weight=None, initial_epoch=0, steps_per_epoch=None, validation_steps=None)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-02T15:43:36.500553Z",
          "start_time": "2019-05-02T15:43:36.347851Z"
        },
        "id": "2Xt6xY0paxyG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(history.history)\n",
        "\n",
        "plt.plot(history.history['loss'], label='train')\n",
        "plt.plot(history.history['val_loss'], label='test')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-02T15:43:44.053319Z",
          "start_time": "2019-05-02T15:43:36.503640Z"
        },
        "id": "Rqg-Snv0axyJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# You can use the early stopped model OR load it. \n",
        "# For that you have to import the load function...\n",
        "# IF AND ONLY IF loading, it is good practice to throw out the trash from the graph...\n",
        "# be.clear_session()\n",
        "\n",
        "result = evaluate_model(model, X_valid_rolled, y_valid_rolled)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-02T15:43:44.057870Z",
          "start_time": "2019-05-02T15:43:44.055366Z"
        },
        "id": "tzVbhKUDaxyO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "assert result < 86.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16MEfBrhaxyQ",
        "colab_type": "text"
      },
      "source": [
        "# Things that should be improved\n",
        "\n",
        "- More conclusive investigation of PACF for better time window estimate\n",
        "    - It can well be, that long windows do not add that much to the performance\n",
        "- More interesting features for XGBoost (like from [tsfresh](https://tsfresh.readthedocs.io/en/latest/)), since present features are a disaster\n",
        "- MOST IMPORTANT: **More thorough error / prediction analysis!!!**\n",
        "- LSTM with **Custom iterator with stateful model**\n",
        "- Investigation of different loss function (eg. MAE) for training. (And with it, think abut the importance of extreme values: do we think they are outliers? Are they interesing to predict?)\n",
        "- Investigation of \"teacher forcing\" for LSTM-s in Keras (if it makes sense)\n",
        "\n",
        "# Conclusion\n",
        "\n",
        "Even with decent amount of struggle, the \"dummy\" of always using the mean is very appealing, so it seems, this is not that easy of a task 24 hours in advance. Further investigation of classical as well as neural models remains open!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JIra4IxcaxyR",
        "colab_type": "text"
      },
      "source": [
        "# Final test\n",
        "\n",
        "We did not use the final test, since our investigations are not concluded yet. Remember: using it once before project \"go live\" is a good practice!"
      ]
    }
  ]
}