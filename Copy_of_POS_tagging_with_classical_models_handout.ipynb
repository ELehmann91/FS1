{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Copy of POS_tagging_with_classical_models_handout.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ELehmann91/FS1/blob/master/Copy_of_POS_tagging_with_classical_models_handout.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "no_k76e0NZHI"
      },
      "source": [
        "# Task: Part of speech tagging\n",
        "\n",
        "In this task we try to recreate a very rudimentary POS tagger \"from scratch\" using SpaCy and CRF models. \n",
        "\n",
        "(We disregard the fact, that SpaCy has a built in POS tagger for the moment for demonstration purposes.)\n",
        "\n",
        "The input is a tokenized English sentence. The task is to label each word with a part of speech (POS) tag. The tag set, which is identical the [Universal Dependencies project's](https://universaldependencies.org/) basic tag set is the following:\n",
        "\n",
        "- NOUN: noun\n",
        "- VERB: verb\n",
        "- DET: determiner\n",
        "- ADJ: adjective\n",
        "- ADP: adposition (e.g., prepositions)\n",
        "- ADV: adverb\n",
        "- CONJ: conjunction\n",
        "- NUM: numeral\n",
        "- PART: particle (function word that cannot be inflected, has no meaning in\n",
        "  itself and doesn't fit elsewhere, e.g., \"to\")\n",
        "- PRON: pronoun\n",
        "- .: punctuation\n",
        "- X: other\n",
        "\n",
        "The code in this task is an adaptation of the NER code in the sklearn-crfsuite documentation.\n",
        "\n",
        "# The data set\n",
        "\n",
        "__Brown__ corpus: \"The Brown University Standard Corpus of Present-Day American English (or just Brown Corpus) was compiled in the 1960s by Henry Kuƒçera and W. Nelson Francis at Brown University, Providence, Rhode Island as a general corpus (text collection) in the field of corpus linguistics. It contains 500 samples of English-language text, totaling roughly one million words, compiled from works published in the United States in 1961\" (Wikpedia: Brown Corpus)\n",
        "\n",
        "Let's download and inspect the data!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WFyA8ugux5S9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%capture\n",
        "!pip install nltk"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-11-12T09:32:20.712982Z",
          "start_time": "2019-11-12T09:32:16.093693Z"
        },
        "colab_type": "code",
        "id": "KAPwh8mmNZHM",
        "outputId": "ced6f51a-687b-4eff-9cfa-4cc98136832c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "import nltk\n",
        "\n",
        "from nltk.corpus import brown\n",
        "nltk.download('brown')\n",
        "\n",
        "brown.words()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', ...]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-11-12T09:32:21.291370Z",
          "start_time": "2019-11-12T09:32:20.723897Z"
        },
        "colab_type": "code",
        "id": "3kSgq4e0NZHi",
        "outputId": "53566098-d02f-406a-9764-b2ebc040514c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "nltk.download('universal_tagset')\n",
        "brown.tagged_words(tagset='universal')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n",
            "[nltk_data]   Package universal_tagset is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('The', 'DET'), ('Fulton', 'NOUN'), ...]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-11-12T09:32:21.352042Z",
          "start_time": "2019-11-12T09:32:21.297210Z"
        },
        "colab_type": "code",
        "id": "9tT1DBTtNZHu",
        "outputId": "1f58b986-cf66-4534-c426-8e8e70448d16",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "brown.sents()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', 'Friday', 'an', 'investigation', 'of', \"Atlanta's\", 'recent', 'primary', 'election', 'produced', '``', 'no', 'evidence', \"''\", 'that', 'any', 'irregularities', 'took', 'place', '.'], ['The', 'jury', 'further', 'said', 'in', 'term-end', 'presentments', 'that', 'the', 'City', 'Executive', 'Committee', ',', 'which', 'had', 'over-all', 'charge', 'of', 'the', 'election', ',', '``', 'deserves', 'the', 'praise', 'and', 'thanks', 'of', 'the', 'City', 'of', 'Atlanta', \"''\", 'for', 'the', 'manner', 'in', 'which', 'the', 'election', 'was', 'conducted', '.'], ...]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-11-12T09:32:25.131849Z",
          "start_time": "2019-11-12T09:32:21.365202Z"
        },
        "colab_type": "code",
        "id": "UuwBB-XRNZH6",
        "outputId": "1e6bc95f-685c-4d29-f4bf-bd902dbbf8bb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(brown.words())"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1161192"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "m8VYZmFCsNTa"
      },
      "source": [
        "From the brown the object provided by NLTK we will work with the tagged sentence list:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-11-12T09:32:25.163453Z",
          "start_time": "2019-11-12T09:32:25.136038Z"
        },
        "colab_type": "code",
        "id": "bh0wAJkWNnlV",
        "outputId": "7f806318-5e3c-4a26-ba6a-e8ea98178f26",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        }
      },
      "source": [
        "sents = brown.tagged_sents(tagset=\"universal\")\n",
        "\n",
        "sents[:1]"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[('The', 'DET'),\n",
              "  ('Fulton', 'NOUN'),\n",
              "  ('County', 'NOUN'),\n",
              "  ('Grand', 'ADJ'),\n",
              "  ('Jury', 'NOUN'),\n",
              "  ('said', 'VERB'),\n",
              "  ('Friday', 'NOUN'),\n",
              "  ('an', 'DET'),\n",
              "  ('investigation', 'NOUN'),\n",
              "  ('of', 'ADP'),\n",
              "  (\"Atlanta's\", 'NOUN'),\n",
              "  ('recent', 'ADJ'),\n",
              "  ('primary', 'NOUN'),\n",
              "  ('election', 'NOUN'),\n",
              "  ('produced', 'VERB'),\n",
              "  ('``', '.'),\n",
              "  ('no', 'DET'),\n",
              "  ('evidence', 'NOUN'),\n",
              "  (\"''\", '.'),\n",
              "  ('that', 'ADP'),\n",
              "  ('any', 'DET'),\n",
              "  ('irregularities', 'NOUN'),\n",
              "  ('took', 'VERB'),\n",
              "  ('place', 'NOUN'),\n",
              "  ('.', '.')]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-11-12T09:32:28.899336Z",
          "start_time": "2019-11-12T09:32:25.166107Z"
        },
        "colab_type": "code",
        "id": "L2oUMrTpasZY",
        "outputId": "54b14e92-0d8f-477e-e398-bcb100461eec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(sents)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "57340"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "tgvacV45sNTz"
      },
      "source": [
        "We divide our data set into a train and a valid part:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-11-12T09:32:28.905030Z",
          "start_time": "2019-11-12T09:32:28.901963Z"
        },
        "colab_type": "code",
        "id": "jPvFic-atE6S",
        "colab": {}
      },
      "source": [
        "valid_sents = sents[:5734]\n",
        "train_sents = sents[5734:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zO8hXBHHPR4f"
      },
      "source": [
        "# Feature template\n",
        "\n",
        "Since the plan is to build a CRF model, we need a __feature template__, which generates features for a word in a sentence (our sequence in the sequence tagging task). We use spaCy for feature extraction."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-11-12T09:32:36.258620Z",
          "start_time": "2019-11-12T09:32:28.906793Z"
        },
        "id": "At1z55Xdx5UD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309
        },
        "outputId": "5de97673-0a12-4668-ec3b-9a5f3acab8b0"
      },
      "source": [
        "! pip install spacy\n",
        "#!python -m spacy download en_core_web_lg\n",
        "#Spacy install, load and such stuff\n",
        "\n",
        "#Import\n",
        "import spacy\n",
        "#By model load, please deactivate unnecessary pipeline elements!\n",
        "\n",
        "en = spacy.load('en_core_web_lg') "
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.6/dist-packages (2.1.9)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.0.3)\n",
            "Requirement already satisfied: plac<1.0.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.9.6)\n",
            "Requirement already satisfied: preshed<2.1.0,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.0.1)\n",
            "Requirement already satisfied: thinc<7.1.0,>=7.0.8 in /usr/local/lib/python3.6/dist-packages (from spacy) (7.0.8)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.17.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.21.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=0.0.6 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.2.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.2)\n",
            "Requirement already satisfied: blis<0.3.0,>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.2.4)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /usr/local/lib/python3.6/dist-packages (from thinc<7.1.0,>=7.0.8->spacy) (4.28.1)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2019.9.11)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "TC4f13w9sNUJ"
      },
      "source": [
        "We write a function which generates features for a token in a sentence, which is already a spaCy document. The feature vector is represented as a `dict` mapping feature names to their values.\n",
        "\n",
        "The desired **feature set for a token is**:\n",
        "\n",
        "- `bias`: A constant value of 1 as an input\n",
        "- `token.lower`: the lowercased textual form of the token\n",
        "- `token.suffix`: the textual form of the token's suffix as defined by SpaCy,\n",
        "- `token.prefix`: the textual form of the token's prefix as defined by SpaCy,\n",
        "- `token.is_upper`: boolean value indicating if the token is uppercase,\n",
        "- `token.is_title`: boolean value indicating if the token is a title,\n",
        "- `token.is_digit`: boolean value indicating if the token consists of numbers.\n",
        "\n",
        "These are only the `Token`'s own properties, but they represent no context.\n",
        "\n",
        "We would like to include information about  the previous and next words, as well as indicating if the `Token` is the beginning or the end of sentence.\n",
        "\n",
        "The **contextual features** should be:\n",
        " \n",
        "- `-1:token.lower`: What is the lowercase textual form of the previous token?,\n",
        "- `-1:token.is_title`: Is the previous token a title?,\n",
        "- `-1:token.is_upper`: Is the previous token uppercase?,\n",
        "- `+1:token.lower`: What is the lowercase textual form of the next token?,\n",
        "- `+1:token.is_title`: Is the next token a title?,\n",
        "- `+1:token.is_upper`: Is the next token uppercase?,\n",
        "- `BOS`: Boolean value indicating if the token is the beginning of a sentence,\n",
        "- `EOS`: Boolean value indicating if the token is the end of a sentence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-11-12T09:32:37.930451Z",
          "start_time": "2019-11-12T09:32:37.909065Z"
        },
        "colab_type": "code",
        "id": "SKz9zT8bsNUL",
        "colab": {}
      },
      "source": [
        "def token2features(sent, i):\n",
        "    \"\"\"Return a feature dict for a token. \n",
        "    sent is a spaCy Doc containing a sentence, i is the token's index in it.\n",
        "    \"\"\"\n",
        "\n",
        "    features = {'bias': 1,\n",
        "                'word' : sent[i].lower_,\n",
        "                'suffix' : sent[i].suffix_,\n",
        "                'prefix' : sent[i].prefix_,\n",
        "                'upper' : sent[i].is_upper,\n",
        "                'title' : sent[i].is_title,\n",
        "                'digit' : sent[i].is_digit\n",
        "                }\n",
        "    if i > 0:\n",
        "        features.update({\n",
        "            'prev_word': sent[i-1].text.lower(),\n",
        "            'prev_title': sent[i-1].is_title,\n",
        "            'prev_upper': sent[i-1].is_upper,\n",
        "\n",
        "        })\n",
        "    else:\n",
        "        features['BOS'] = True\n",
        "        #features['prev_word'] = None\n",
        "        #features['prev_title'] = None\n",
        "        #features['prev_upper'] = None\n",
        "\n",
        "    if i < len(sent)-1:\n",
        "        features.update({\n",
        "            'next_word': sent[i+1].text.lower(),\n",
        "            'next_title': sent[i+1].is_title,\n",
        "            'next_upper': sent[i+1].is_upper,\n",
        "\n",
        "        })\n",
        "    else:\n",
        "        features['EOS'] = True\n",
        "        #features['next_word'] = None\n",
        "        #features['next_title'] = None\n",
        "        #features['next_upper'] = None\n",
        "\n",
        "    return features\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VXljxG047l6r",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9f9996b2-30ca-4966-8db5-99cc72e92a04"
      },
      "source": [
        "sentence = en('I have a immutable large car')\n",
        "sentence"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "I have a immutable large car"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZX6blJ737l-_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "19b9204b-254f-4c0e-e856-33df1527fdd8"
      },
      "source": [
        "token2features(sentence,0)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'BOS': True,\n",
              " 'bias': 1,\n",
              " 'digit': False,\n",
              " 'next_title': False,\n",
              " 'next_upper': False,\n",
              " 'next_word': 'have',\n",
              " 'prefix': 'I',\n",
              " 'suffix': 'I',\n",
              " 'title': True,\n",
              " 'upper': True,\n",
              " 'word': 'i'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "VvwL0hF3sNUS"
      },
      "source": [
        "For training, we will also need functions to generate feature dict and label lists for sentences in our training corpus:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-11-12T09:32:37.958184Z",
          "start_time": "2019-11-12T09:32:37.936592Z"
        },
        "colab_type": "code",
        "id": "ZLW80wtksNUU",
        "colab": {}
      },
      "source": [
        "from spacy.tokens import Doc\n",
        "\n",
        "def sent2features(sent):\n",
        "    \"Return a list of feature dicts for a sentence in the data set.\"\n",
        "    # Create a doc by instantiating a Doc class and iterating through the sentence token by token.\n",
        "    # Please bear in mind, that Brown has token-POS pairs, latter one we don't need here...\n",
        "\n",
        "    words  = [a for a,b in sent]\n",
        "\n",
        "    doc = Doc(en.vocab, words=words)\n",
        "\n",
        "    # Plese use the above defined token2features function on each token to generate the features\n",
        "    # For the whole sentence!\n",
        "    sent_features= [token2features(doc,i) for i,word in enumerate(doc)]\n",
        "    \n",
        "    return sent_features\n",
        "\n",
        "def sent2labels(sent):\n",
        "    \n",
        "    #Please create / filter only the labels for given sentence!\n",
        "    labels= [b for a,b in sent]\n",
        "    \n",
        "    return labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "pBoPuzeMsNUa"
      },
      "source": [
        "Sanity check: let's see the values for the first 2 tokens in the corpus:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-11-12T09:32:37.997140Z",
          "start_time": "2019-11-12T09:32:37.966347Z"
        },
        "colab_type": "code",
        "id": "UcqvDIJofccv",
        "outputId": "bfc8b3ab-2d9a-4609-8027-673106a08cad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "for i in range(80,82):\n",
        "  print(sents[i])\n",
        "  print(sent2features(sents[i])[:2])\n",
        "  print(sent2labels(sents[i])[:2])"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('It', 'PRON'), ('says', 'VERB'), ('that', 'ADP'), ('``', '.'), ('in', 'ADP'), ('the', 'DET'), ('event', 'NOUN'), ('Congress', 'NOUN'), ('does', 'VERB'), ('provide', 'VERB'), ('this', 'DET'), ('increase', 'NOUN'), ('in', 'ADP'), ('federal', 'ADJ'), ('funds', 'NOUN'), (\"''\", '.'), (',', '.'), ('the', 'DET'), ('State', 'NOUN'), ('Board', 'NOUN'), ('of', 'ADP'), ('Education', 'NOUN'), ('should', 'VERB'), ('be', 'VERB'), ('directed', 'VERB'), ('to', 'PRT'), ('``', '.'), ('give', 'VERB'), ('priority', 'NOUN'), (\"''\", '.'), ('to', 'ADP'), ('teacher', 'NOUN'), ('pay', 'NOUN'), ('raises', 'NOUN'), ('.', '.')]\n",
            "[{'bias': 1, 'word': 'it', 'suffix': 'It', 'prefix': 'I', 'upper': False, 'title': True, 'digit': False, 'BOS': True, 'next_word': 'says', 'next_title': False, 'next_upper': False}, {'bias': 1, 'word': 'says', 'suffix': 'ays', 'prefix': 's', 'upper': False, 'title': False, 'digit': False, 'prev_word': 'it', 'prev_title': True, 'prev_upper': False, 'next_word': 'that', 'next_title': False, 'next_upper': False}]\n",
            "['PRON', 'VERB']\n",
            "[('Colquitt', 'NOUN')]\n",
            "[{'bias': 1, 'word': 'colquitt', 'suffix': 'itt', 'prefix': 'C', 'upper': False, 'title': True, 'digit': False, 'BOS': True, 'EOS': True}]\n",
            "['NOUN']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "KsoK0-GNfzt5"
      },
      "source": [
        "# Putting the data into final form"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Ylfst7VGsNUl"
      },
      "source": [
        "Everything is ready to generate the training data in the form which is usable for the CRFsuite. Note that our inputs and labels will be  2-level representations, lists of lists, because we deal with token sequences (sentences)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-11-12T09:33:02.066506Z",
          "start_time": "2019-11-12T09:32:38.005545Z"
        },
        "colab_type": "code",
        "id": "Wfqa0feYgspT",
        "outputId": "d9691de0-6515-4e04-d5a5-951b5c9a8d7f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "%%time\n",
        "X_train = [sent2features(s) for s in train_sents]\n",
        "y_train = [sent2labels(s) for s in train_sents]\n",
        "\n",
        "X_valid = [sent2features(s) for s in valid_sents]\n",
        "y_valid = [sent2labels(s) for s in valid_sents]"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 37.2 s, sys: 777 ms, total: 37.9 s\n",
            "Wall time: 38 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-11-12T09:33:02.072026Z",
          "start_time": "2019-11-12T09:33:02.068258Z"
        },
        "colab_type": "code",
        "id": "XNFvu0UosNUt",
        "outputId": "f4ed2510-e8f2-4876-a350-7c7990e0c358",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "print(\"Feature dict for the first token in the first validation sentence:\")\n",
        "print(X_valid[0][0])\n",
        "print(\"Its label:\")\n",
        "print(y_valid[0][0])"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Feature dict for the first token in the first validation sentence:\n",
            "{'bias': 1, 'word': 'the', 'suffix': 'The', 'prefix': 'T', 'upper': False, 'title': True, 'digit': False, 'BOS': True, 'next_word': 'fulton', 'next_title': True, 'next_upper': False}\n",
            "Its label:\n",
            "DET\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "f2siQxe9k4ql"
      },
      "source": [
        "# Training and evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xwyn356ysNU2"
      },
      "source": [
        "We use the super-optimized [CRFsuite](http://www.chokkan.org/software/crfsuite/) via the scikit-learn compatible [sklearn-crfsuite](https://sklearn-crfsuite.readthedocs.io) wrapper to train a CRF model on the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-11-12T09:33:04.830327Z",
          "start_time": "2019-11-12T09:33:02.073675Z"
        },
        "colab_type": "code",
        "id": "15POzt86sNSe",
        "colab": {}
      },
      "source": [
        "%%capture \n",
        "# only to avoid ugly printouts during install\n",
        "!pip install sklearn_crfsuite"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-11-12T09:33:04.883741Z",
          "start_time": "2019-11-12T09:33:04.836395Z"
        },
        "colab_type": "code",
        "id": "WkX57BFDklEL",
        "colab": {}
      },
      "source": [
        "# Please import and train an averaged perceptron model from CRFsuite and use it's custom metrics, \n",
        "# especially the multiple forms of accuracy score to evaluate the model!\n",
        "import sklearn_crfsuite \n",
        "from sklearn_crfsuite import scorers\n",
        "from sklearn_crfsuite import metrics"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dpAWhYteeiih",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "64d821da-14fb-4c94-8832-ad0a5a3d8cf0"
      },
      "source": [
        "%%time\n",
        "crf = sklearn_crfsuite.CRF(\n",
        "    algorithm='lbfgs',\n",
        "    c1=0.1,\n",
        "    c2=0.1,\n",
        "    max_iterations=100,\n",
        "    all_possible_transitions=True\n",
        ")\n",
        "crf.fit(X_train, y_train)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 3min 5s, sys: 372 ms, total: 3min 5s\n",
            "Wall time: 3min 5s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sTLTM6RJx5Ud",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "abc48c09-43c6-4760-d332-ed36684b9f53"
      },
      "source": [
        "# Please draw some conclusion if this model is \"good enough\" \n",
        "# in your view if you take token level and sentence level metrics into account!\n",
        "\n",
        "labels = list(crf.classes_)\n",
        "#labels.remove('O')\n",
        "labels"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['VERB',\n",
              " 'ADP',\n",
              " 'ADV',\n",
              " '.',\n",
              " 'DET',\n",
              " 'NOUN',\n",
              " 'ADJ',\n",
              " 'PRT',\n",
              " 'CONJ',\n",
              " 'NUM',\n",
              " 'PRON',\n",
              " 'X']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m8TjVycSgG_P",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d8051f15-6d5e-441a-8e35-7fdfa520d72e"
      },
      "source": [
        "y_pred = crf.predict(X_valid)\n",
        "metrics.flat_f1_score(y_valid, y_pred,\n",
        "                      average='weighted', labels=labels)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9767205077886142"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PDKJTFUUyl9R",
        "colab_type": "text"
      },
      "source": [
        "Yes ist good enaugth!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NnIsK2sAmyKl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0c3efcd6-c270-4709-c514-55f01b826fc6"
      },
      "source": [
        "metrics.sequence_accuracy_score(y_valid, y_pred)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6510289501220788"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PEy73c3ltAJg",
        "colab_type": "text"
      },
      "source": [
        "Oops on sentence level only 65% Accuracy, thats not that good."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E3G6S3digHCn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# group B and I results\n",
        "sorted_labels = sorted(\n",
        "    labels,\n",
        "    key=lambda name: (name[1:], name[0])\n",
        ")\n",
        "print(metrics.flat_classification_report(\n",
        "    y_valid, y_pred, labels=sorted_labels, digits=3\n",
        "))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "63p9RtDhsNU_"
      },
      "source": [
        "Let's instantiate and fit our model. CRFsuite implements several learning methods, here we use \"ap\", i.e., averaged perceptron."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "li8CXg67sNVc"
      },
      "source": [
        "# Demonstration\n",
        "\n",
        "Just for the fun, we can try out the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-11-12T09:35:17.983727Z",
          "start_time": "2019-11-12T09:35:17.965723Z"
        },
        "colab_type": "code",
        "id": "JHoYAGHFsNVe",
        "colab": {}
      },
      "source": [
        "def predict_tags(sent):\n",
        "    \"\"\"Predict tags for a sentence.\n",
        "    sent is a string.\n",
        "    \"\"\"\n",
        "    doc = en(sent)\n",
        "    return crf.predict([[token2features(doc, i) for i in range(len(doc))]])\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-11-12T09:35:37.676093Z",
          "start_time": "2019-11-12T09:35:17.986500Z"
        },
        "colab_type": "code",
        "id": "Ya59xso_z7uj",
        "outputId": "5bcc6aef-1293-4670-d3b0-7cbe3bf8ea13",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        " while True:\n",
        "        sent = input(\"\\nEnter a sentence to tag or press return to quit:\\n\")\n",
        "        if sent:\n",
        "            print(predict_tags(sent))\n",
        "        else:\n",
        "            print(\"\\nEmpty input received -- bye!\")\n",
        "            break"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Enter a sentence to tag or press return to quit:\n",
            "Hello is it me you are looking for?\n",
            "[['PRT', 'VERB', 'PRON', 'PRON', 'PRON', 'VERB', 'VERB', 'ADP', '.']]\n",
            "\n",
            "Enter a sentence to tag or press return to quit:\n",
            "\n",
            "\n",
            "Empty input received -- bye!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "23ylch22zdvn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}